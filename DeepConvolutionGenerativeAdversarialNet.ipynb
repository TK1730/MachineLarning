{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのインポート\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2a1e68b8950>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定\n",
    "workers = 0\n",
    "batch_size = 50\n",
    "nz = 100\n",
    "nch_g = 128\n",
    "nch_d = 128\n",
    "n_epoch = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "outf = './result_3_2-DCGAN'\n",
    "display_interval = 600\n",
    "\n",
    "# 保存先ディレクトリを作成\n",
    "try:\n",
    "    os.makedirs(outf, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "    pass\n",
    "\n",
    "# 乱数のシード（種）を固定\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTの訓練データセットを読み込む\n",
    "dataset = dset.MNIST(root='data/mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize((0.5,), (0.5,))\n",
    "                     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 画像配列の確認\n",
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 画像の域値(-1~1)の確認\n",
    "print(dataset[0][0].min())\n",
    "print(dataset[0][0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 訓練データをセットしたデータローダーを作成する\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# 学習に使用するデバイスを得る．可能ならGPUを使用する\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器GのクラスGenerator\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    生成器Gのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nz=100, nch_g=128, nch=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            nz (int, optional): 入力ベクトルzの次元数. Defaults to 100.\n",
    "            nch_g (int, optional): 最終層の入力チャンネル数. Defaults to 128.\n",
    "            nch (int, optional): 出力画像のチャンネル数. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # ニューラルネットワークの構造定義\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nz, nch_g * 4, kernel_size=3, stride=1, padding=0),  # 転置畳み込み\n",
    "                nn.BatchNorm2d(nch_g * 4),  # バッチノーマライゼーション\n",
    "                nn.ReLU(),  # 活性化関数 ReLU\n",
    "            ),  # (B, nz, 1, 1) -> (B, nch_g*4, 3, 3)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 4, nch_g * 2, kernel_size=3, stride=2, padding=0),\n",
    "                nn.BatchNorm2d(nch_g * 2),\n",
    "                nn.ReLU(),\n",
    "            ),  # (B, nch_g*4, 3, 3) -> (B, nch_g*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 2, nch_g, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(nch_g),\n",
    "                nn.ReLU(),\n",
    "            ),  # (B, nch_g, 14, 14) -> (B, nch, 14, 14)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g, nch, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            )   # (B, nch_g, 14, 14) >- (B, nch, 28, 28)\n",
    "        })\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        Args:\n",
    "            z : 入力ベクトル\n",
    "\n",
    "        Returns:\n",
    "            生成画像\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersnの各層で演算を行う\n",
    "            z = layer(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別器DのクラスDiscriminator\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    識別器Dのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nch=1, nch_d=128):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.Conv2d(nch, nch_d, 4, 2, 1),  # 畳み込み\n",
    "                nn.LeakyReLU(negative_slope=0.2),  # leaky ReLU\n",
    "            ), # (B, nch, 28, 28) >- (B, nch_d, 14, 14)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nch_d * 2),\n",
    "                nn.LeakyReLU(negative_slope=0.2),\n",
    "            ),  # (B, nch_d, 14, 14) >- (B, nch_d*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 2, nch_d * 4, 3, 2, 0),\n",
    "                nn.BatchNorm2d(nch_d * 4),\n",
    "                nn.LeakyReLU(negative_slope=0.2),\n",
    "            ),  # (B, nch_d*2, 7, 7) >- (B, nch_d*4, 3, 3)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 4, 1, 3, 1, 0),\n",
    "                nn.Sigmoid(), # 活性関数 Sigmoid関数\n",
    "            )  # (B, nch_d*4, 3, 3) >- (B, 1, 1, 1)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        Args:\n",
    "            x : 本物画像あるいは生成画像\n",
    "\n",
    "        Returns:\n",
    "            識別信号\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersの各層で演算を行う\n",
    "            x = layer(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    ニューラルネットワークの重みを初期化する。作成したインスタンスに対しapplyメソッドで適用する\n",
    "    Args:\n",
    "        m : ニューラルネットワークを構成する層\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:    # 畳み込み層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:  # 全結合層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:  # バッチノーマライゼーション場合\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): ConvTranspose2d(100, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 生成器G ランダムベクトルから生成画像を作成する\n",
    "netG = Generator(nz=nz, nch_g=nch_g).to(device)\n",
    "netG.apply(weights_init)  # weights_init関数で初期化\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1            [-1, 512, 3, 3]         461,312\n",
      "       BatchNorm2d-2            [-1, 512, 3, 3]           1,024\n",
      "              ReLU-3            [-1, 512, 3, 3]               0\n",
      "   ConvTranspose2d-4            [-1, 256, 7, 7]       1,179,904\n",
      "       BatchNorm2d-5            [-1, 256, 7, 7]             512\n",
      "              ReLU-6            [-1, 256, 7, 7]               0\n",
      "   ConvTranspose2d-7          [-1, 128, 14, 14]         524,416\n",
      "       BatchNorm2d-8          [-1, 128, 14, 14]             256\n",
      "              ReLU-9          [-1, 128, 14, 14]               0\n",
      "  ConvTranspose2d-10            [-1, 1, 28, 28]           2,049\n",
      "             Tanh-11            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 2,169,473\n",
      "Trainable params: 2,169,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.98\n",
      "Params size (MB): 8.28\n",
      "Estimated Total Size (MB): 9.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 生成器GのTensor形状\n",
    "torchsummary.summary(netG, (100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 識別器D 画像が本物画像が生成画像かを識別する\n",
    "netD = Discriminator(nch_d=nch_d).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 14, 14]           2,176\n",
      "         LeakyReLU-2          [-1, 128, 14, 14]               0\n",
      "            Conv2d-3            [-1, 256, 7, 7]         524,544\n",
      "       BatchNorm2d-4            [-1, 256, 7, 7]             512\n",
      "         LeakyReLU-5            [-1, 256, 7, 7]               0\n",
      "            Conv2d-6            [-1, 512, 3, 3]       1,180,160\n",
      "       BatchNorm2d-7            [-1, 512, 3, 3]           1,024\n",
      "         LeakyReLU-8            [-1, 512, 3, 3]               0\n",
      "            Conv2d-9              [-1, 1, 1, 1]           4,609\n",
      "          Sigmoid-10              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 1,713,025\n",
      "Trainable params: 1,713,025\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.78\n",
      "Params size (MB): 6.53\n",
      "Estimated Total Size (MB): 7.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 識別器DのTensor形状\n",
    "torchsummary.summary(netD, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # バイナリークロスエントロピー (sigmoid関数無し)\n",
    "\n",
    "# 生成器のエポックごとの画像生成に使用する確認用の固定ノイズ\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# オプティマイザ―のセットアップ\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 識別器D用\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 生成器G用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][1/1200] Loss_D: 1.315 Loss_G: 3.030 D(x): 0.577 D(G(z)): 0.467/0.059\n",
      "[1/10][601/1200] Loss_D: 0.582 Loss_G: 2.951 D(x): 0.819 D(G(z)): 0.267/0.071\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m netD(fake_image)  \u001b[38;5;66;03m# 更新した識別器Dで改めて生成画像に対する識別信号を出力\u001b[39;00m\n\u001b[0;32m     46\u001b[0m errG \u001b[38;5;241m=\u001b[39m criterion(output, real_target)  \u001b[38;5;66;03m# 生成器Gの損失値。Dに生成画像を本物画像と誤認させたいため目標値は[1]\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43merrG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 誤差逆伝搬\u001b[39;00m\n\u001b[0;32m     48\u001b[0m D_G_z2 \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# 更新した識別器Dによる生成画像の識別信号の平均\u001b[39;00m\n\u001b[0;32m     50\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Gのパラメータを更新\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kazu\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kazu\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kazu\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "D_x_out = []\n",
    "D_G_z1_out = []\n",
    "\n",
    "# 学習のループ\n",
    "for epoch in range(n_epoch):\n",
    "    for itr, data in enumerate(dataloader):\n",
    "        # 本物画像\n",
    "        real_image = data[0].to(device)\n",
    "        # 画像枚数\n",
    "        sample_size = real_image.size(0)\n",
    "\n",
    "        # 標準正規分布からノイズを生成\n",
    "        noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
    "        # 本物画像に対する識別信号の目標値 [1]\n",
    "        real_target = torch.full((sample_size, ), 1., device=device)\n",
    "        # 生成画像に対する識別信号の目標値 [0]\n",
    "        fake_target = torch.full((sample_size, ), 0., device=device)\n",
    "\n",
    "        ##############################\n",
    "        # 識別器Dの更新\n",
    "        ##############################\n",
    "        netD.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        output = netD(real_image)  # 識別器Dで本物画像に対する識別信号を出力\n",
    "        errD_real = criterion(output, real_target)  # 本物画像に対する識別信号の損失\n",
    "        D_x = output.mean().item()  # 生成画像の識別信号の平均\n",
    "\n",
    "        fake_image = netG(noise)  # 生成器Gでノイズから生成画像を生成\n",
    "\n",
    "        output = netD(fake_image.detach())  # 識別器Dで生成画像に対する識別信号を出力\n",
    "        errD_fake = criterion(output, fake_target)  # 生成画像に対する識別信号の損失値\n",
    "        D_G_z1 = output.mean().item()  # 生成画像の識別信号の平均\n",
    "\n",
    "        errD = errD_real + errD_fake  # 識別器Dの全体の損失\n",
    "        errD.backward()  # 誤差逆伝搬\n",
    "        optimizerD.step()  # Dのパラメーターを更新\n",
    "\n",
    "        ##############################\n",
    "        # 生成器Gの更新\n",
    "        ##############################\n",
    "        netG.zero_grad()  # 勾配の初期化\n",
    "\n",
    "        output = netD(fake_image)  # 更新した識別器Dで改めて生成画像に対する識別信号を出力\n",
    "        errG = criterion(output, real_target)  # 生成器Gの損失値。Dに生成画像を本物画像と誤認させたいため目標値は[1]\n",
    "        errG.backward()  # 誤差逆伝搬\n",
    "        D_G_z2 = output.mean().item()  # 更新した識別器Dによる生成画像の識別信号の平均\n",
    "\n",
    "        optimizerG.step()  # Gのパラメータを更新\n",
    "\n",
    "        if itr % display_interval == 0:\n",
    "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
    "                  .format(epoch + 1, n_epoch, itr + 1, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        if epoch == 0 and itr == 0:\n",
    "            vutils.save_image(real_image, '{}/real_samples.png'.format(outf), normalize=True, nrow=10)\n",
    "\n",
    "        # ログ出力用データの保存\n",
    "        D_losses.append(errD.item())\n",
    "        G_losses.append(errG.item())\n",
    "        D_x_out.append(D_x)\n",
    "        D_G_z1_out.append(D_G_z1)\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "    \n",
    "    ##############################\n",
    "    # 確認用画像の生成\n",
    "    ##############################\n",
    "    fake_image = netG(fixed_noise)  # 1エポック終了ごとに確認用の生成画像を生成する\n",
    "    vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1), normalize=True, nrow=10)\n",
    "\n",
    "    ##############################\n",
    "    # モデルの保存\n",
    "    ##############################\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "        torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器G.ランダムベクトルから生成画像を生成する\n",
    "netG = Generator(nz=nz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
